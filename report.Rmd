---
title: "Assignement 1 Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
### Ring

I have implemented an MPI program in C with $2P$ messages passing among $P$ processes on a ring topology with periodic boundaries.

The program implements a stream of messages of $4B$ each, both clockwise and anticlockwise: each process in fact send/receive messages of **type int** (the $rank$) with a tag proportional to the rank ( $tag=rank*10$ ).

When running on $P$ process, the program prints out in folder *./out* a file *'np*$P$*.txt'* with the following output ordered by rank (here $P=5$):

`I am process 0 and I have received 5 messages. My final messages have tag 0 and value msg-left -10, msg-right 10`
`I am process 1 and I have received 5 messages. My final messages have tag 10 and value msg-left -10, msg-right 10`
`I am process 2 and I have received 5 messages. My final messages have tag 20 and value msg-left -10, msg-right 10`
`I am process 3 and I have received 5 messages. My final messages have tag 30 and value msg-left -10, msg-right 10`
`I am process 4 and I have received 5 messages. My final messages have tag 40 and value msg-left -10, msg-right 10`

I have used the following routines for message passing among processes:

* `MPI_Isend`: **non-blocking send** routine
* `MPI_Irecv`: **non-blocking receive** routine
* `MPI_Wait`: waits for a non-blocking operation to complete

Using latest version of OpenMPI available on ORFEO (openmpi-4.1.1+gnu-9.3.0), I have run the program up to $P=24$ processes on a **thin node with InfiniBand network and native protocol**.

I have taken notes of the runtime (from the first send/receive to the last) when varying the number of processes $P$ by means of the routine `MPI_Wtime()` which returns an elapsed time on the calling process in seconds.

At each run with $P$ processes, for each process I have taken the mean of the **runtime in microseconds** out of **10000** iterations and print it out (along with some statistics) by ranking order on the file *'np*$P$*.csv'* in folder *./out*, as follows:
```{r include=FALSE}
P5_results=read.csv("img/P5_results.csv", header = TRUE)
7.072714/5
7.073540/5
7.072797/5
7.074060/5
```

```{r table, echo=FALSE, results='asis'}
knitr::kable(
  head(P5_results, 6), booktabs = TRUE,
  caption = 'Results with P = 5'
)
```

Then, by taking for each run with different $P$ only the **maximum value of t_mean** between all processes as measure of the performance, I was able to produce the following plot, running the program with different mappings:

```{r, include=FALSE}
t_default=read.csv("./section1/mydata/ring/default/results.csv", header = TRUE)
t_by_socket=read.csv("./section1/mydata/ring/by_socket/results.csv", header = TRUE)
t_by_node=read.csv("./section1/mydata/ring/by_node/results.csv", header = TRUE)
t_by_core=read.csv("./section1/mydata/ring/by_core/results.csv", header = TRUE)
```

```{r, include=FALSE}
col_default <- "#4885ed"
col_by_socket <- "#3cba54"
col_by_node <- "#f4c20d"
col_by_core <- "#db3236"
```

```{r, echo=FALSE, fig.align='center'}
#plotting
library(ggplot2)
sp = ggplot(t_default,aes(x=Np,y=mean,color="default")) +
  scale_x_continuous(name="P processes",breaks=t_default$Np)  +
  theme(text = element_text(size=12)) +
  geom_point(size = 4,shape=17) + geom_line()+scale_y_continuous(breaks = seq(0,70,5),name = expression(bold("t ("*mu*"s)")))   +
  geom_point(data=t_by_core,aes(x=Np,y=mean,color="by_core"),size = 4,shape=17) +
  geom_line(data=t_by_core,aes(x=Np,y=mean,color="by_core")) +
  geom_point(data=t_by_socket,aes(x=Np,y=mean,color="by_socket"),size = 4,shape=17) +
  geom_line(data=t_by_socket,aes(x=Np,y=mean,color="by_socket")) +
  geom_point(data=t_by_node,aes(x=Np,y=mean,color="by_node"),size = 4,shape=17) +
  geom_line(data=t_by_node,aes(x=Np,y=mean,color="by_node")) +
  scale_color_manual(name="Run", values = c(col_by_core, col_by_node, col_by_socket, col_default))+
  scale_linetype_manual(values=c(1))+
  labs(title="Ring on THIN node", subtitle="Execution time per slower process vs number of processes") +
  theme(plot.title = element_text(size = 15, face = "bold",hjust = 0.5,margin = margin(t = 6)), plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(t = 4, b = 7), face = "italic"), axis.title = element_text(face = "bold"), legend.text = element_text(margin = margin(t = 7, b = 7, r=6)), axis.title.x = element_text(margin = margin(t = 10, b=6)), axis.title.y = element_text(margin = margin(r = 8,l=6)), axis.text = element_text(color= "#2f3030", face="bold")) +
  theme(
#  panel.background = element_rect(fill = "#ebeef0", colour = "#d5dce0",
#                               size = 2, linetype = "solid"),
  panel.grid.major = element_line(size = 0.3, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.17, linetype = 'solid',
                                colour = "white")
  )

sp

```

Because of the routines that I have used in the program, **I expect my data to be in compliance with a $P$ double PingPing model**. 

By means of **IMB-MPI1 PingPing benchmark** it's possible to measure startup $\Delta t$ and throughput $\frac{X}{\Delta t}$ of single messages of size $X$ that are obstructed by oncoming messages. To achieve this, two processes communicate with each other using *MPI_Isend/MPI_Recv/MPI_Wait* calls, just like in my program, as follows:

```{r, echo=FALSE, warning=FALSE, message = FALSE, out.width="40%", fig.cap="Fig.1 PingPing Pattern from Intel® MPI Benchmarks User Guide and Ring with P=3.",fig.align='center',fig.show='hold'}
library(magick)
ring_ping <- image_read("./img/ring_ping.png")
ring_ping_with_border <- image_border(ring_ping, "white", "40x10")
image_write(ring_ping_with_border, "./img/ring_ping_with_border.png")
knitr::include_graphics(c("./img/pingping.png","./img/ring_ping_with_border.png"))
```

With IMB-MPI1 PingPing benchmark I was able to estimate the latency $\lambda_{net}$ and bandwidth $b_{net}$ on Infiniband network with different processes mappings: across nodes, sockets and cores.
Since my $t_{mean}$ is a measure of time in $\mu s$ of a pair of opposite messages passing through all $P$ process till returning back to the original one, $t_{xmsg}=\frac{t_{mean}}{P}$ is the variable accounting for half of the $\Delta t$ time measured from the PingPing benchmark results.

Thus, by taking the inverse of the previous relation $t_{mean}=t_{xmsg}P$, our theoretical model will be $t_{theo}=2\Delta t_{ping}{P}$, where $\Delta t_{ping} = \lambda_{ping} + \frac{4B}{b_{ping}}$ with $\lambda_{ping}$ and $b_{ping}$ estimated by least square method on the PingPing benchmark results.

Hence, the following communication model has been used for plotting my data with the PingPing model:

* $t(P)=2P(\lambda_{core} + \frac{4B}{b_{core}})$ for **core mapping when $P\leq 12$**.
* $t(P)=2P(\lambda_{socket} + \frac{4B}{b_{socket}})$ for **core mapping when $P=13$**, assuming that the process chosen as representative of my running is the one with the maximum value of $t_{mean}$, hence the one wich is in the other socket farther from the others 12 cores.
* $t(P)=P(\lambda_{core} + \frac{4B}{b_{core}})+P(\lambda_{socket} + \frac{4B}{b_{socket}})$ for **core mapping when $P>13$**, assuming again that the slower process is the one communicating with the previous core which is in the other socket, and the next core which is inside its socket.
* $t(P)=2P(\lambda_{socket} + \frac{4B}{b_{socket}})$ for **socket mapping**.
* $t(P)=2P(\lambda_{core} + \frac{4B}{b_{core}})$ for **node mapping**.

```{r, include=FALSE}
ring_model_core <- t_by_core
ring_model_socket <- t_by_socket
ring_model_node <- t_by_node
ring_model_node2 <- t_by_node
core_latency=0.264
socket_latency=0.485
node_latency=0.996
b_core <- 6372.991
b_socket <- 5530.801
b_node <- 11945.604
msg_size <- 4/10^6

msg_size/b_core
msg_size/b_socket
msg_size/b_node
core_latency + (msg_size/b_core)
socket_latency + (msg_size/b_socket)
core_jump <- ring_model_core$mean[ring_model_core$Np==13] - ring_model_core$mean[ring_model_core$Np==12]
ring_model_core$mean[ring_model_core$Np<=12]=ring_model_core$Np[ring_model_core$Np<=12]*(core_latency + (msg_size/b_core))*2
ring_model_core$mean[ring_model_core$Np>12]=ring_model_core$Np[ring_model_core$Np>12]*(socket_latency + (msg_size/b_socket)) + ring_model_core$Np[ring_model_core$Np>12]*(core_latency + (msg_size/b_core))

ring_model_core$mean[ring_model_core$Np==13]=ring_model_core$Np[ring_model_core$Np==13]*(socket_latency + (msg_size/b_socket))*2

ring_model_socket$mean=ring_model_socket$Np*(socket_latency + (msg_size/b_socket))*2

ring_model_node$mean=ring_model_node$Np*(node_latency + (msg_size/b_node))*2

ring_model_node2$mean=ring_model_node2$Np*(1.35 + (msg_size/b_node))
```

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.align='center', out.width="100%"}
lab_core <- as.character(expression('t(N)'=='2N'*(lambda[cor]+frac(4*'B', b[cor]))))
lab_core2 <- as.character(expression('t(N)'=='N'*(lambda[cor]+frac(4*'B', b[cor]))+'N'*(lambda[soc]+frac(4*'B', b[soc]))))
lab_socket <- as.character(expression('t(N)'=='2N'*(lambda[soc]+frac(4*'B', b[soc]))))
lab_node <- as.character(expression('t(N)'=='2N'*(lambda[nod]+frac(4*'B', b[nod]))))
lab_node2 <- as.character(expression('t(N)'=='N'*(1.35 +frac(4*'B', b[nod]))))

sp = ggplot(t_by_core,aes(x=Np,y=mean,color="by core",linetype="Exp data"))  +
  scale_y_continuous(name= expression(bold("t ("*mu*"s)")),breaks = seq(0,45,by=5))+
  scale_x_continuous(name="P processes",breaks = t_by_core$Np) +
  geom_point(size = 3,shape=17) + 
  geom_line()  + 
  theme(text = element_text(size=10)) +
  geom_line(data=ring_model_core,aes(x=Np,y=mean,color="by core",linetype="Model")) +    
  geom_point(data=t_by_socket,aes(x=Np,y=mean,color="by socket"),size = 3,shape=17) +
  geom_line(data=t_by_socket,aes(x=Np,y=mean,color="by socket",linetype="Exp data"))+
  geom_line(data=ring_model_socket,aes(x=Np,y=mean,color="by socket",linetype="Model"))+
  geom_line(data=ring_model_node,aes(x=Np,y=mean,color="by node",linetype="Model"))+
  geom_point(data=t_by_node,aes(x=Np,y=mean,color="by node"),size = 3,shape=17) +
  geom_line(data=t_by_node,aes(x=Np,y=mean,color="by node",linetype="Exp data"))+
  geom_line(data=ring_model_node2,aes(x=Np,y=mean,linetype="Model"), color="#104ec9")+
  scale_color_manual(name="Run", values = c(col_by_core, col_by_node, col_by_socket, col_default))+
  scale_linetype_manual(name="Data",values=c(1,2))+
  geom_text(x = 8, y = 2, label = lab_core, color = col_by_core, size=2, parse = T) + 
  geom_text(x = 18, y = 10.7, label = lab_core2, color = col_by_core, size=2, parse = T) +
  geom_text(x = 16, y = 18, label = lab_socket, color = col_by_socket, size=2, parse = T) +
  geom_text(x = 15, y = 27, label = lab_node, color = "#ffbf00", size=2, parse = T) +
  geom_text(x = 21, y = 25, label = lab_node2, color = "#104ec9", size=2, parse = T) +
  theme(plot.title = element_text(size = 12, face = "bold",hjust = 0.5,margin = margin(t = 6)), plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(t = 4, b = 7), face = "italic"), axis.title = element_text(face = "bold"), legend.text = element_text(margin = margin(t = 4, b = 4, r=4), size=8), legend.title = element_text(size = 9), axis.title.x = element_text(margin = margin(t = 8, b=6)), axis.title.y = element_text(margin = margin(r = 10,l=8)), axis.text = element_text(color= "#2f3030", face="bold"), plot.caption = element_text(color = "darkblue", face = "italic", size = 10, margin = margin(b =6))) +
 labs(title="Ring comparison with PingPing model of 4B message", subtitle="Execution time vs number of processes", caption = bquote(paste(lambda[cor], "= ", .(core_latency), ",  ", lambda[soc], "= ", .(socket_latency), ", ",  lambda[nod], "= ", .(node_latency)))) +
 theme(
#  panel.background = element_rect(fill = "#ebeef0", colour = "#d5dce0",
#                               size = 2, linetype = "solid"),
  panel.grid.major = element_line(size = 0.3, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.17, linetype = 'solid',
                                colour = "white")
  )
sp
```

Oss. The extimated bandwiths of the network for the different mappings are:

* $b_{core}$ = 6372.991 MB/s
* $b_{socket}$ = 5530.801 MB/s
* $b_{node}$ = 11945.604 MB/s

As expected, the measured bandwith for the 100 Gbit InfiniBand network (process message passing among nodes) reaches up to approximately 12000 MB/s ($= 12$ GB/s $= 12 *8$ Gbit/s $= 96$ Gbit/s) which is $96\%$ of the theoretical peak performance.
Anyway this term in the model can easily be omitted since it's in the order of $10^{-10}$.

The estimated latency between two nodes $\lambda_{node}$ with the PingPing benchmark is somewhat less than the one declared by Mellanox switch constructor ($1.35$ $\mu s$) used in InfiniBand network: this fact seems to hold when few processes are communicating.

I was indeed expecting that the theoretical performance (evaluated by PingPing benchmark among just 2 process) would have been better than the real scenario when all cores/sockets in nodes are being used simultaneously, and in fact as can be seen from the previous plot, **my model seems to work good for core and socket mapping**, but quite bad for node mapping.

Data from node mapping seem to follow another model that is half the expected one: $t(P)=P(\lambda_{mlx5} + \frac{4B}{b_{node}})$, where $\lambda_{mlx5}=1.35$ is the latency of the Mellanox switch indeed. This may suggest that the *switch is able to send simultaneously 2 messages of $4B$ in each direction of the network, halving the time for message passing* between processes placed in different nodes.

## Exercise 2
### Measure MPI point to point performance

The **Intel MPI benchmark PingPong** has been used to estimate latency $\lambda_{net}$ and bandwidth $b_{net}$ of *all available combinations of topologies and networks on ORFEO computational nodes, using both IntelMPI and openmpi latest versions* libraries availables.

Let's start by looking at ORFEO computational nodes and resources:

```{bash pbsnodes, eval=FALSE, warning=FALSE }
[valinsogna@login 2021Assignement01]$ pbsnodes -ajS
                                                        mem       ncpus   nmics   ngpus
vnode           state           njobs   run   susp      f/t        f/t     f/t     f/t   jobs
--------------- --------------- ------ ----- ------ ------------ ------- ------- ------- -------
ct1pf-fnode001  job-busy             1     1      0    560gb/1tb    0/36     0/0     0/0 56793
ct1pf-fnode002  free                 1     1      0      1tb/1tb   12/36     0/0     0/0 55267
ct1pt-tnode001  job-busy             1     1      0   54gb/754gb    0/24     0/0     0/0 56845
ct1pt-tnode002  job-busy             1     1      0   54gb/754gb    0/24     0/0     0/0 56846
ct1pt-tnode004  job-busy             1     1      0   54gb/754gb    0/24     0/0     0/0 56847
ct1pt-tnode005  job-busy             1     1      0   54gb/754gb    0/24     0/0     0/0 56794
ct1pt-tnode006  free                 0     0      0  754gb/754gb   24/24     0/0     0/0 --
ct1pt-tnode007  job-busy             1     1      0   54gb/754gb    0/24     0/0     0/0 56795
ct1pt-tnode008  job-busy             1     1      0   54gb/754gb    0/24     0/0     0/0 56848
ct1pt-tnode009  job-busy             1     1      0  754gb/754gb    0/24     0/0     0/0 57073
ct1pt-tnode010  job-busy             1     1      0  754gb/754gb    0/24     0/0     0/0 57073
ct1pt-tnode003  offline              0     0      0  754gb/754gb   24/24     0/0     0/0 --
ct1pg-gnode001  free                 1     1      0  252gb/252gb   44/48     0/0     0/0 57057
ct1pg-gnode002  free                 1     1      0  252gb/252gb   24/48     0/0     0/0 56354
ct1pg-gnode003  free                 1     1      0  252gb/252gb   24/48     0/0     0/0 56862
ct1pg-gnode004  free                 0     0      0  252gb/252gb   48/48     0/0     0/0 --
```

On ORFEO there are:

- 2 fat nodes: with 2 CPUs (2 NUMA domains) of 18 cores each, with more than 1 TB of RAM.
- 4 gpu nodes: with hyper-threading enabled, with 2 CPUs (2 NUMA domains) of 12 physical cores each with more than 252 GB of RAM.
- 10 thin nodes: with 2 CPUs (2 NUMA domains) of 12 cores each, with more than 754 GB of RAM.
- login node: with 2 CPUs (2 NUMA domains) of 10 cores each.

**The MPI point to point performance has been measured only on thin and gpu nodes**.

### Node topology
Now we can look at the node topologies for the thin and gpu ones. 
This can be either done by typing on the selected node `module load likwid` and then `likwid-topology` or using `module load hwloc` and  `lstopo`.
The figure below represents the lstopo output on a thin node, which is more or less the same for the gpu node. 

```{r, echo=FALSE, out.width="55%", fig.cap="Fig.2 Topology on thin node", fig.align='center'}
knitr::include_graphics("./img/lstopo.png")
```

The main differences between a gpu and a thin node are the hyper-threading enabled on the previous, the different RAM size and the different CPUs models:

* Intel(R) Xeon(R) Gold 6226 CPU @ **2.70GHz** for gpu node
* Intel(R) Xeon(R) Gold 6126 CPU @ **2.60GHz** for thin node

As can be seen by typing `lscpu`:

```{bash lscputhin, eval=FALSE, warning=FALSE }
[valinsogna@ct1pt-tnode008 ~]$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                24
On-line CPU(s) list:   0-23
Thread(s) per core:    1
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 85
Model name:            Intel(R) Xeon(R) Gold 6126 CPU @ 2.60GHz
Stepping:              4
CPU MHz:               3299.999
CPU max MHz:           3700.0000
CPU min MHz:           1000.0000
BogoMIPS:              5200.00
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              19712K
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23
```

```{bash lscpugpu, eval=FALSE, warning=FALSE }
[valinsogna@ct1pg-gnode001 ~]$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 85
Model name:            Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz
Stepping:              7
CPU MHz:               3499.999
CPU max MHz:           3700.0000
CPU min MHz:           1200.0000
BogoMIPS:              5400.00
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              19712K
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47
```


### Networks

On ORFEO there are two physical networks that have been used for performance measurements: 

- High Speed Network **100 Gbit InfiniBand**;
- In band management network **25 Gbit Ethernet**;

The main differences among these two are the usage of Sockets Interface and TCP/IP protocol for the Ethernet network, and the usage of much more rapid OpenFabrics Verbs (no Kernel stack) with the native IB protocol for InfiniBand network.
Moreover, IB protocol has RDMA (Remote Direct Memory Access) that makes InfiniBand faster: it is an operation which access the memory directly  without involving the CPU.
We thus, expect InfiniBand network with native IB protocol to be much faster (low $\lambda_{net}$, high $b_{net}$) then the Ethernet network.
Moreover, I am going to test also InfiniBand network performance when applying plain IP protocol: IPoIB (IP-over-InfiniBand) is the protocol that defines how to send IP packets over IB, passing through the Kernel space.

As can be seen from Fig.2, the network cards are placed in each node only inside one of the two NUMA domains, and there are several PCI (Peripherical Component Interconnect) devices that can be seen by typing `ifconfig`:


```{bash ifconfig, eval=FALSE }
[valinsogna@ct1pt-tnode007 ~]$ ifconfig
bond0: flags=5187<UP,BROADCAST,RUNNING,MASTER,MULTICAST>  mtu 1500
        inet6 fe80::3680:dff:fe4e:5568  prefixlen 64  scopeid 0x20<link>
        ether 34:80:0d:4e:55:68  txqueuelen 1000  (Ethernet)

br0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 10.128.2.127  netmask 255.255.255.0  broadcast 10.128.2.255
        inet6 fe80::3680:dff:fe4e:5568  prefixlen 64  scopeid 0x20<link>
        ether 34:80:0d:4e:55:68  txqueuelen 1000  (Ethernet)

em1, em2 two physical cards that we have on the node
em1: flags=6211<UP,BROADCAST,RUNNING,SLAVE,MULTICAST>  mtu 1500
        ether 34:80:0d:4e:55:68  txqueuelen 1000  (Ethernet)

em2: flags=6211<UP,BROADCAST,RUNNING,SLAVE,MULTICAST>  mtu 1500
        ether 34:80:0d:4e:55:68  txqueuelen 1000  (Ethernet)

ib0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 2044
        inet 10.128.6.127  netmask 255.255.255.0  broadcast 10.128.6.255
        inet6 fe80::ba59:9f03:d4:27d6  prefixlen 64  scopeid 0x20<link>
Infiniband hardware address can be incorrect! Please read BUGS section in ifconfig(8).
        infiniband 00:00:09:07:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00  txqueuelen 256  (InfiniBand)

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
```

As from above, **em1, em2 are two physical distinguished Ethernet cards that refers to one interface br0**, whilst **ib0 is InfiniBand** network.
More details are shown below with `lstopo`:

```{bash lstopo2, eval=FALSE }
[valinsogna@ct1pt-tnode007 ~]$ lstopo
Machine (754GB total)
  Package L#0
    NUMANode L#0 (P#0 376GB)
    L3 L#0 (19MB)
      L2 L#0 (1024KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)
      L2 L#1 (1024KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#2)
      ...
    HostBridge
      PCI 00:11.5 (SATA)
      PCI 00:17.0 (SATA)
      PCIBridge
        PCIBridge
          PCI 03:00.0 (VGA)
    HostBridge
      PCIBridge
        PCI 18:00.0 (RAID)
          Block(Disk) "sda"
      PCIBridge
        PCI 19:00.0 (Ethernet)
          Net "em1"
        PCI 19:00.1 (Ethernet)
          Net "em2"
    HostBridge
      PCIBridge
        PCI 3b:00.0 (InfiniBand)
          Net "ib0"
          OpenFabrics "mlx5_0"
```

With openMPI implementation and UCX, it's possible to directly select the devices (using UCX_NET_DEVICES specification in the run command), that lead to a specific protocol as consequence. **The devices tested are:**

* **ib0: IPoIB protocol.**
* **br0: TCP communication, Ethernet.**
* **mlx5_0:1: native IB protocol.**

The theoretical maximum performance are $12.5$ GB/s or $12800$ MB/s for InfiniBand and $3.125$ GB/s or $3200$ MB/s for the Ethernet network. 

By Intel's documentation it works as follow:

```{r, echo=FALSE, out.width="35%", fig.cap="PingPong Pattern from Intel® MPI Benchmarks User Guide", fig.align='center'}
knitr::include_graphics("./img/pingping.png")
```

The bandwidth and the latency estimate is done across *core*, *socket* and different *node*, combined with differents protocols and hardware devices. Each run is repeated 10 times, **openMPI 4.0.3** is used. Entire node was reserved in order to reduce possible source noise in measurement.

The **pml** involved in the benchmarks are **OB1** and **UCX**. The **btl** used are **tcp** and **vader**. Across node are selected also different network, with different protocols: $25$ Gbit Ethernet and $100$ Gbit InfiniBand throught Mellanox network switch. 

The following graphs show the behaviour inside the same node, i.e mapping the processes across two sockets or in the same socket. Mapping the processes in the same socket show of course often a better performance. The behaviour before the asymptotic plateau is strange and show very different performance among different implementation, the main cause is the cache. 

Cache sizes are reported on graph with vertical black dashed line. 
The behaviour appear strange before $16$ MB included, after that size, it is stable beacause the message size is larger than all caches. The larger cache is **L3** with $19$ MB.
The **L2** effects, become clear after $1$ MB, then all implementations start loosing bandwidth.
The **L1** effects is not clearly visible from the bandwidth due the latency. To infer with more accuracy about the cache effect we can perform some profiling test using hardware counters. I modified Intel MPI Benchmark PingPong injecting some code in order to inspect L1 data cache misses and L2 cache misses using PAPI. I tracked all cache misses on MPI_Send and MPI_Recv routine of rank 0,after some cicles of warmup. They are reported on absolute value and normalized respect message size.
Before $128$ KB UCX shows poor performance respect other protocols, after that size there is an huge speed up. This behave will be explained later. Intel InfiniBand also show poor performance respect UCX implementation, this behaviour is explained with cache too. 
